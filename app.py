import streamlit as st
from pdf_processing import extract_text_from_pdf, chunk_text
from url_processing import extract_text_from_url
from llm import generate_answer
from vector_db import FAISSVectorDB
from embeddings import create_embeddings
from openai import OpenAI

client = OpenAI()

st.title("Knowledge Base Bot")

# Input selector
input_type = st.radio("Select input type", ["PDF Upload", "URL"])
user_question = st.text_input("Ask a question:")

text = None
if input_type == "PDF Upload":
    uploaded_file = st.file_uploader("Upload a PDF", type="pdf")
    if uploaded_file:
        text = extract_text_from_pdf(uploaded_file)

elif input_type == "URL":
    url = st.text_input("Enter URL")
    if url:
        text = extract_text_from_url(url)

if text and user_question:
    with st.spinner("üîç Reading, indexing, and answering..."):
        # 1Ô∏è‚É£ Chunk text
        chunks = chunk_text(text)

        if not chunks:
            st.error("No readable content found.")
            st.stop()

        # 2Ô∏è‚É£ Generate embeddings (SAFE + throttled)
        embeddings = create_embeddings(chunks)

        if not embeddings:
            st.error("Failed to generate embeddings.")
            st.stop()

        # 3Ô∏è‚É£ Store in FAISS
        vector_db = FAISSVectorDB(dimension=len(embeddings[0]))
        vector_db.add_embeddings(embeddings, chunks[:len(embeddings)])

        # 4Ô∏è‚É£ Embed user question
        query_embedding = client.embeddings.create(
            model="text-embedding-3-large",
            input=user_question
        ).data[0].embedding

        relevant_chunks = vector_db.query(query_embedding, top_k=3)

        # 5Ô∏è‚É£ Generate answer
        answer = generate_answer(user_question, relevant_chunks)

    st.subheader("Answer")
    st.write(answer)
